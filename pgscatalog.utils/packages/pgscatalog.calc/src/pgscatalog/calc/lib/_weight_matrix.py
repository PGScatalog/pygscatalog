from __future__ import annotations

import logging
from typing import TYPE_CHECKING

import duckdb
import numpy as np
import polars as pl
import zarr

from .constants import ZARR_VARIANT_CHUNK_SIZE

if TYPE_CHECKING:
    import numpy.typing as npt

    from .types import Pathish


logger = logging.getLogger(__name__)


def create_wide_weights_table(
    db_path: Pathish, sampleset: str, accessions: list[str]
) -> list[str]:
    """
    Create a weight matrix and supporting information in the wide_weights_table table.

    Long form = one row per variant per accession
    Wide form = one row per variant, one column per accession

    Parameters
    ----------
    db_path : Pathish
        Path to the DuckDB database file.
    sampleset : str
        Identifier for the set of samples to pivot.

    Notes
    -----
    - Null effect weights are filled with 0.
    - A variant may appear multiple times if different `effect_allele_idx`
      values apply (e.g. REF/ALT vs ALT/REF matches)
    """
    with duckdb.connect(str(db_path)) as conn:
        logger.info(f"Creating long score temporary table for {sampleset=}")
        # create a temporary table first to enforce data integrity
        conn.sql("""
            CREATE OR REPLACE TEMPORARY TABLE long_scores_temp (
                sampleset TEXT NOT NULL,
                accession TEXT NOT NULL,
                is_recessive BOOL NOT NULL,
                is_dominant BOOL NOT NULL,
                -- UBIGINT: 64 bit integer for fancy indexing with numpy / zarr
                row_nr UBIGINT NOT NULL,
                target_row_nr UBIGINT NOT NULL,
                -- UINT8: indexes are always teeny tiny, save space in these big arrays
                effect_allele_idx UINT8,
                filename VARCHAR,
                effect_weight DOUBLE NOT NULL,
                -- each score variant must only have one row
                PRIMARY KEY (accession, row_nr)
            );
        """)

        logger.info("Inserting score data into long scores temporary table")
        conn.execute(
            """
        INSERT INTO long_scores_temp
        SELECT COALESCE(m.sampleset, $sampleset) AS sampleset,
               s.accession,
               s.effect_type = 'recessive' AS is_recessive,
               s.effect_type = 'dominant' AS is_dominant,
               s.row_nr,
               m.target_row_nr,
               m.match_result.effect_allele_idx AS effect_allele_idx,
               m.filename,
               s.effect_weight
        FROM score_variant_table s
        JOIN allele_match_table m ON s.row_nr = m.row_nr
            AND s.accession = m.accession
            AND m.sampleset = $sampleset
        WHERE effect_allele_idx IS NOT NULL AND
            m.accession IN $accession
        ORDER BY m.target_row_nr;
        """,
            {"sampleset": sampleset, "accession": accessions},
        )

        logger.info("Pivoting long scores to create weight matrix")
        conn.sql("""
            CREATE OR REPLACE TABLE wide_score_variants AS
            WITH pivoted_scores AS (
                PIVOT long_scores_temp
                ON accession
                /*
                - SQL PIVOT from long to wide requires an aggregation function
                - but the pivoted values (effect weights) are unique on accession
                - any_value is a workaround, it's no-op (does nothing)
                - could also use SUM or MEAN and the result would be the same
                - coalesce fills NULLs generated by the pivot with zero
                */
                USING coalesce(any_value(effect_weight), 0)
                /*
                - same target_row_nr with different effect allele idx will make new rows
                - _this is really important to calculate dosage correctly_
                */
                GROUP BY
                    sampleset,
                    filename,
                    is_recessive,
                    is_dominant,
                    target_row_nr,
                    effect_allele_idx
                ORDER BY target_row_nr
            )
            SELECT
                row_number() OVER ()  - 1 AS weight_mat_row_nr,
                concat_ws('/', sampleset, filename) AS zarr_group,
                * EXCLUDE(sampleset, filename)
            FROM pivoted_scores;
            """)

        logger.info("Pivot finished, checking data integrity ")
        conn.sql("""
            /*
            data integrity check after pivoting:
            - weight data should have been added as columns, so each row must be a
              single target_row_nr in a zarr group
            - however, it's OK for target_row_nr to have multiple rows if the effect
              allele index is different
            - this can happen when score 1 matches REFALT and score 2 matches ALTREF
              at the same variant
            */
            ALTER TABLE wide_score_variants ADD PRIMARY KEY (
                zarr_group,
                target_row_nr,
                effect_allele_idx
            );
        """)

        zarr_groups = conn.sql("""
        SELECT DISTINCT zarr_group
        FROM wide_score_variants;
        """).fetchall()

        return [x for xs in zarr_groups for x in xs]


def store_group_weight_arrays(
    db_path: Pathish, sampleset: str, zarr_group: zarr.Group, accessions: list[str]
) -> dict[str, pl.DataFrame]:
    """Generate weight matrices for each zarr group by pivoting scoring file weights
    from long to wide form.

    Long form = one row per variant per accession
    Wide form = one row per variant, one column per accession

    This function materialises a staging table in DuckDB, pivots it, validates variant
    uniqueness, and yields weight matrices for each distinct Zarr group.

    Parameters
    ----------
    db_path : Pathish
        Path to the DuckDB database file.
    sampleset : str
        Identifier for the set of samples to pivot.
    zarr_group : zarr.Group
        Writable zarr group for this sampleset
    accessions : list[str]
        List of accessions to pivot (must pass match rate)

    Returns
    -------
    dict[str, pl.DataFrame]
        A dictionary mapping each zarr group name to its index metadata dataframe,
        which is used to calculate dosage.
    """
    # reset zarr group state when this function is called
    for array in zarr_group.array_keys():
        logger.info(f"Deleting {array=}")
        del zarr_group[array]

    group_paths = create_wide_weights_table(
        db_path=db_path, sampleset=sampleset, accessions=accessions
    )

    metadata_dfs = []
    for group in group_paths:
        store_results_in_zarr(db_path=db_path, group_path=group, zarr_group=zarr_group)
        metadata_dfs.append(get_variant_metadata(db_path=db_path, group_path=group))

    logger.info("Finished storing arrays in zarr")

    if not metadata_dfs:
        raise ValueError(f"{zarr_group=} {group_paths=} returned empty dataframe")
    logger.info("Creating dataframe with variant/effect allele index metadata")
    grouped_metadata_dfs = pl.concat(metadata_dfs).partition_by(
        "zarr_group", as_dict=True, include_key=False
    )

    # partition_by returns keys as tuples, extract and explicitly convert to strings
    return {str(k[0]): v for k, v in grouped_metadata_dfs.items()}


def store_results_in_zarr(
    db_path: Pathish, group_path: str, zarr_group: zarr.Group
) -> None:
    """
    Store a weight matrix for a specific Zarr group in a zarr array.

    This function extracts pivoted effect weights for all accessions within the
    given `zarr_group` from the `wide_score_variants` table.

    Parameters
    ----------
    conn : duckdb.DuckDBPyConnection
        Active DuckDB connection containing the `wide_score_variants` table.
    group_path : str
        Identifier for the Zarr group. This value corresponds to the
        `sampleset/filename` combination used during pivoting.
    """
    logger.info(f"Getting weight matrix for {group_path=}")

    accessions, weights = get_weight_matrix(db_path=db_path, group_path=group_path)

    try:
        # initialise the array and metadata
        zarr_group.create_array(
            "weight_matrix",
            overwrite=False,
            data=weights,
            chunks=(ZARR_VARIANT_CHUNK_SIZE, weights.shape[1]),
        )
        zarr_group.attrs["accessions"] = accessions
    except zarr.errors.ContainsArrayError:
        weight_mat = zarr_group["weight_matrix"]
        if not isinstance(weight_mat, zarr.Array):
            raise TypeError("weight_matrix must be a zarr array") from None

        weight_mat.append(weights)

    logger.info("Finished writing to zarr store")


def get_variant_metadata(db_path: Pathish, group_path: str) -> pl.DataFrame:
    """
    Retrieve index metadata for a specific zarr group from the wide score variants table

    The returned dataframe contains one row per variant, with metadata required to
    calculate dosage and align it with the weight matrix

    Parameters
    ----------
    db_path : Pathish
        Path to the DuckDB database file.
    group_path : str
        Identifier of the zarr group to query.

    Returns
    -------
    pl.DataFrame
        Dataframe with the following columns:
        - zarr_group : str
        - weight_mat_row_nr : int64
        - effect_allele_idx : uint8
        - target_row_nr : int64
        - is_recessive : bool
        - is_dominant: bool

        Rows are ordered by target_row_nr to be consistent with the effect weight
        matrix.
    """
    with duckdb.connect(str(db_path), read_only=True) as conn:
        return conn.execute(
            """
            SELECT zarr_group,
                   weight_mat_row_nr,
                   effect_allele_idx,
                   target_row_nr,
                   is_recessive,
                   is_dominant
            FROM wide_score_variants
            WHERE zarr_group = $zarr_group
            ORDER BY target_row_nr
            """,
            {"zarr_group": group_path},
        ).pl()


def get_weight_matrix(
    db_path: Pathish, group_path: str
) -> tuple[list[str], npt.NDArray[np.float64]]:
    """
    Extract the weight matrix for a specific zarr group in wide form.

    The wide matrix has one row per variant/effect allele and one column per accession.

    Metadata columns (such as indices) are excluded.

    Parameters
    ----------
    db_path : Pathish
        Path to the DuckDB database file.
    group_path : str
        Identifier of the zarr group to query.

    Returns
    -------
    tuple[list[str], numpy.ndarray]
        - accessions : list of str
            Names of the accessions, corresponding to the array columns.
        - matrix : ndarray of shape (n_variants, n_accessions), dtype float64
            Stacked weight values, ordered by target_row_nr.
    """
    with duckdb.connect(str(db_path), read_only=True) as conn:
        query = conn.execute(
            """
            SELECT * EXCLUDE (weight_mat_row_nr,
                zarr_group,
                effect_allele_idx,
                target_row_nr,
                is_recessive,
                is_dominant)
            FROM wide_score_variants
            WHERE zarr_group = $zarr_group
            ORDER BY target_row_nr
            """,
            {"zarr_group": group_path},
        ).fetchnumpy()
        accessions = [str(key) for key in query]
        return accessions, np.stack(list(query.values()), axis=1)
